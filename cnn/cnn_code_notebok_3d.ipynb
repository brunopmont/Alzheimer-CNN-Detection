{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Conv3D, MaxPooling3D, Flatten, Dense, Dropout, BatchNormalization, Activation, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import seaborn as sns\n",
    "\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from PIL import Image\n",
    "import tempfile\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÕES\n",
    "# Winsorize -> reduz outliers, limitando os percentis inf e sup\n",
    "def winsorize_image(image_data, lower_percentile=1, upper_percentile=99): #reduz valores extremos\n",
    "    lower_bound = np.percentile(image_data, lower_percentile)\n",
    "    upper_bound = np.percentile(image_data, upper_percentile)\n",
    "    winsorized_data = np.clip(image_data, lower_bound, upper_bound)\n",
    "    return winsorized_data\n",
    "\n",
    "# Normalization -> valores de voxels entre 0 e 1\n",
    "def normalize_image_min(image_data): \n",
    "    min_val = np.min(image_data)\n",
    "    max_val = np.max(image_data)\n",
    "    normalized_data = (image_data - min_val) / (max_val - min_val)\n",
    "    return normalized_data\n",
    "\n",
    "# Função para carregar imagens NIfTI, seus rótulos e cortar as imagens\n",
    "def load_nifti_paths(base_dir, class_names):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    # Caminhos das subpastas\n",
    "    for label in class_names:\n",
    "        label_dir = os.path.join(base_dir, label)\n",
    "        for fname in os.listdir(label_dir):\n",
    "            img_path = os.path.join(label_dir, fname)\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(label)\n",
    "\n",
    "    # Codificando os rótulos\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Inverter a ordem das classes explicitamente\n",
    "    label_encoder.classes_ = np.array(class_names)\n",
    "\n",
    "    # Convertendo a lista de rótulos para um array NumPy\n",
    "    labels_array = np.array(labels)\n",
    "\n",
    "    # Codificando os rótulos (agora 'cn' será 0 e 'ad' será 1)\n",
    "    labels_encoded = label_encoder.transform(labels_array)\n",
    "\n",
    "    # Transformando os rótulos para one-hot encoding\n",
    "    labels_one_hot = to_categorical(labels_encoded, num_classes=len(class_names))\n",
    "\n",
    "    # Embaralhar os dados\n",
    "    image_paths, labels_one_hot = shuffle(image_paths, labels_one_hot, random_state=42)\n",
    "\n",
    "    return image_paths, labels_one_hot, label_encoder.classes_\n",
    "\n",
    "# Função para carregar imagens NIfTI, seus rótulos e cortar as imagens\n",
    "def nifti_data_generator_3d(image_paths, labels, batch_size, shuffle_each_epoch=False):\n",
    "    while True:\n",
    "        if shuffle_each_epoch:\n",
    "            image_paths, labels = shuffle(image_paths, labels, random_state=42)\n",
    "\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i + batch_size]\n",
    "            batch_labels = labels[i:i + batch_size]\n",
    "            images = []\n",
    "            \n",
    "            for path in batch_paths:\n",
    "                # Carregar a imagem NIfTI e garantir o formato correto\n",
    "                img = nib.load(path).get_fdata()  # Shape original: \n",
    "                img = img[..., np.newaxis]       # Adicionar a dimensão do canal: \n",
    "                images.append(img)\n",
    "            \n",
    "            # Converter lista para array NumPy e garantir o shape correto\n",
    "            images = np.array(images) \n",
    "            batch_labels = np.array(batch_labels)\n",
    "\n",
    "            # Liberar memória\n",
    "            gc.collect()\n",
    "            \n",
    "            yield images, batch_labels\n",
    "\n",
    "\n",
    "# Função para carregar imagens NIfTI, seus rótulos e cortar as imagens\n",
    "def nifti_data_generator_pdf(image_paths, labels, batch_size, shuffle_each_epoch=False):\n",
    "    while True:\n",
    "        if shuffle_each_epoch == True:\n",
    "            image_paths, labels = shuffle(image_paths, labels, random_state=42)\n",
    "\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i + batch_size]\n",
    "            batch_labels = labels[i:i + batch_size]\n",
    "            images = []\n",
    "            \n",
    "            for path in batch_paths:\n",
    "                img = nib.load(path).get_fdata()\n",
    "                images.append(img)\n",
    "            \n",
    "            images = np.array(images).reshape((-1, *img.shape, 1))\n",
    "            batch_labels = np.array(batch_labels)\n",
    "\n",
    "            gc.collect()\n",
    "            \n",
    "            yield images, batch_labels, batch_paths\n",
    "\n",
    "def plot_training_history(history, dir):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Graphic')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['categorical_accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_categorical_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Graphic')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(os.path.join(dir, 'training_history.png'))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, dir):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(os.path.join(dir, 'confusion_matrix.png'))\n",
    "    plt.show()\n",
    "\n",
    "def create_model_3d(input_shape, n_classes):\n",
    "    model = Sequential([        \n",
    "        Input(shape=input_shape),  # Formato de entrada: (1, 145, 182, 7)\n",
    "\n",
    "        # Camada 1 - Filtro 3x3\n",
    "        Conv3D(32, (3, 3, 3), padding='same', kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(negative_slope=0.3),  \n",
    "        MaxPooling3D(pool_size=(1, 2, 2), padding='same'),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        # Camada 2 - Filtro 5x5\n",
    "        Conv3D(64, (3, 3, 3), padding='same', kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(negative_slope=0.3),  \n",
    "        MaxPooling3D(pool_size=(1, 2, 2), padding='same'),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        # Camada 3 - Filtro 3x3\n",
    "        Conv3D(128, (3, 3, 3), padding='same', kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(negative_slope=0.3),  \n",
    "        MaxPooling3D(pool_size=(2, 2, 2), padding='same'),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        # Camada 4 - Filtro 5x5\n",
    "        Conv3D(256, (3, 3, 3), padding='same', kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(negative_slope=0.3),  \n",
    "        MaxPooling3D(pool_size=(1, 2, 2), padding='same'), # talvez reduzir mais uma vez\n",
    "        Dropout(0.4),\n",
    "\n",
    "        # Camada de saída convolucional\n",
    "        Flatten(),\n",
    "\n",
    "        # Camadas densas\n",
    "        Dense(256, kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        LeakyReLU(negative_slope=0.3),  \n",
    "\n",
    "        Dense(128, kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        LeakyReLU(negative_slope=0.3),  \n",
    "\n",
    "        # Camada de saída\n",
    "        Dense(n_classes, activation='softmax')  # Sem regularização L2 na camada de saída\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo caminhos\n",
    "dir_base = r\"C:\\Users\\Team Taiane\\Desktop\\ADNI\\FULL_ADNI\\processed_7_slices_data\\7_slices_axial\"\n",
    "\n",
    "train_dir = f'{dir_base}/train'\n",
    "val_dir = f'{dir_base}/validation'\n",
    "test_dir = f'{dir_base}/test'\n",
    "results_dir = f'{dir_base}/results/3d'\n",
    "\n",
    "# Criar o diretório de resultados se ele não existir\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome das classes\n",
    "class_names = ['cn', 'emci', 'mci', 'lmci', 'ad']\n",
    "\n",
    "n_classes = len(class_names)\n",
    "\n",
    "train_image_paths, train_labels, class_labels = load_nifti_paths(train_dir, class_names)\n",
    "val_image_paths, val_labels, _ = load_nifti_paths(val_dir, class_names)\n",
    "test_image_paths, test_labels, _ = load_nifti_paths(test_dir, class_names)\n",
    "\n",
    "print(f\"N treino: {len(train_image_paths)}\")\n",
    "print(f\"N validation: {len(val_image_paths)}\")\n",
    "print(f\"N validation: {len(test_image_paths)}\")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "steps_per_epoch = len(train_image_paths) // batch_size\n",
    "validation_steps = len(val_image_paths) // batch_size\n",
    "\n",
    "train_generator = nifti_data_generator_3d(train_image_paths, train_labels, batch_size, shuffle_each_epoch=False)\n",
    "val_generator = nifti_data_generator_3d(val_image_paths, val_labels, batch_size)\n",
    "test_generator = nifti_data_generator_3d(test_image_paths, val_labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(os.listdir(results_dir))\n",
    "\n",
    "if (n > 0):\n",
    "    if (len(os.listdir(os.path.join(results_dir, f'test_{n}'))) < 3): \n",
    "        for item in os.listdir(os.path.join(results_dir, f\"test_{n}\")):\n",
    "            os.remove(os.path.join(results_dir,  f\"test_{n}\", item))\n",
    "        os.removedirs(os.path.join(results_dir, f'test_{n}'))\n",
    "        n -= 1\n",
    "\n",
    "folder_name = f\"test_{str(n+1)}\"\n",
    "results_dir = os.path.join(results_dir, folder_name)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "print(f\"pasta {folder_name} criada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compila model\n",
    "input_shape_x = (7, 182, 155, 1) # sagital\n",
    "input_shape_y = (145, 7, 155, 1) # coronal\n",
    "input_shape_z = (145, 182, 7, 1) # axial\n",
    "\n",
    "model = create_model_3d(input_shape_x, n_classes)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 120\n",
    "\n",
    "new_model_name_ker = (f\"binary_classifier_{epochs}_epochs_batch_{batch_size}_{n_classes}_classes.keras\")\n",
    "\n",
    "# Supondo que você já tenha um modelo e esteja treinando com `fit`\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',     \n",
    "    patience=15,                 \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Defina o nome do arquivo para salvar o melhor modelo\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=os.path.join(results_dir, new_model_name_ker),    \n",
    "    monitor='val_categorical_accuracy',\n",
    "    save_best_only=True, \n",
    "    mode='max', \n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "\n",
    "log_path = os.path.join(results_dir, 'log_treino.csv')\n",
    "\n",
    "csv_log = CSVLogger(log_path, append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Iniciando treinamento do modelo {new_model_name_ker} para classes {class_names}\")\n",
    "\n",
    "# Treinamento\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=val_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[model_checkpoint_callback, reduce_lr, csv_log]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pdf_generator = nifti_data_generator_pdf(val_image_paths, val_labels, batch_size, shuffle_each_epoch=False)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_image = []\n",
    "y_paths = []\n",
    "\n",
    "best_model = load_model(os.path.join(results_dir, new_model_name_ker))\n",
    "\n",
    "for i in range(0, ceil(len(val_image_paths) / batch_size)):\n",
    "    batch_images, batch_labels, batch_paths = next(val_pdf_generator)\n",
    "    y_image.append(batch_images)\n",
    "    y_true.append(batch_labels)\n",
    "    y_paths.append(batch_paths)\n",
    "    \n",
    "    # Fazendo predição para o lote atual\n",
    "    batch_pred = best_model.predict(batch_images)\n",
    "    y_pred.append(batch_pred)\n",
    "\n",
    "# Concatenando as predições e os rótulos verdadeiros\n",
    "y_paths = np.concatenate(y_paths)\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "y_image = np.concatenate(y_image)\n",
    "\n",
    "# Convertendo as predições para rótulos (a classe com maior probabilidade)\n",
    "y_true_labels = np.argmax(y_true, axis=1)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para carregar uma imagem NIfTI e extrair uma fatia específica do eixo Z\n",
    "def load_nifti_image_pdf(file_path):\n",
    "    img = nib.load(file_path) \n",
    "    data = img.get_fdata()  \n",
    "    slice_2d = data[2, :, :]\n",
    "\n",
    "    # implementar lógica de retornar vetor com cada fatia como imagem única\n",
    "\n",
    "    return slice_2d\n",
    "\n",
    "# Função para criar o PDF\n",
    "def create_pdf(y_paths, y_true_labels, y_pred_labels, output_pdf_path):\n",
    "    c = canvas.Canvas(output_pdf_path, pagesize=letter)\n",
    "    width, height = letter  # Dimensões da página no PDF\n",
    "\n",
    "    i = 0\n",
    "    for item in y_paths:\n",
    "        true = ''\n",
    "        pred = ''\n",
    "        # Carregar a imagem NIfTI e obter a fatia 2D no eixo Z\n",
    "        nifti_image = load_nifti_image_pdf(item)\n",
    "        \n",
    "        # Converter a fatia 2D para uma imagem 8-bit (grayscale) para visualização\n",
    "        img = Image.fromarray(np.uint8(nifti_image / np.max(nifti_image) * 255))  # Normalizar e converter\n",
    "        img = img.convert(\"RGB\")  # Garantir que a imagem tenha 3 canais (RGB)\n",
    "\n",
    "        # Redimensionar a imagem para se ajustar ao tamanho da página\n",
    "        img_width, img_height = img.size\n",
    "        aspect_ratio = img_height / float(img_width)\n",
    "        new_width = width * 0.2  # Definir largura como 80% da largura da página\n",
    "        new_height = new_width * aspect_ratio\n",
    "        img = img.resize((int(new_width), int(new_height)))\n",
    "\n",
    "        # Criar um arquivo temporário para salvar a imagem\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as temp_file:\n",
    "            temp_file_path = temp_file.name\n",
    "            img.save(temp_file_path)\n",
    "\n",
    "        # configurar para printar as 7 fatias em uma página inteira, com as informações de label predito e esperado\n",
    "\n",
    "        # Colocar a imagem no PDF usando o caminho temporário\n",
    "        if i % 12 < 4:\n",
    "            x = 80\n",
    "        elif i % 12 < 8:\n",
    "            x = width - 2.35*new_width - 80\n",
    "        else:\n",
    "            x = width - new_width - 80\n",
    "\n",
    "        y = height - (new_height + 80)*((i%4)+1)\n",
    "\n",
    "        c.drawImage(temp_file_path, x, y, width=new_width, height=new_height)\n",
    "\n",
    "        # Escrever os rótulos\n",
    "        true_label = y_true_labels[i]\n",
    "        pred_label = y_pred_labels[i]\n",
    "\n",
    "        #ver como transformar os labels de maneira inteligente\n",
    "        true = class_names[true_label]\n",
    "        pred = class_names[pred_label]\n",
    "\n",
    "        # Definir a cor para os rótulos\n",
    "        if true_label == pred_label:\n",
    "            pred_color = (0, 1, 0)  # Verde\n",
    "        else:\n",
    "            pred_color = (1, 0, 0)  # Vermelho\n",
    "        \n",
    "        #Nome paciente (em preto)\n",
    "        c.setFont(\"Helvetica\", 12)\n",
    "        c.setFillColorRGB(0, 0, 0)  # Preto\n",
    "        c.drawString(x+24, y+new_height+50, f\"{os.path.basename(item)}\")\n",
    "\n",
    "        # Rótulo esperado (em preto)\n",
    "        c.setFont(\"Helvetica\", 12)\n",
    "        c.setFillColorRGB(0, 0, 0)  # Preto\n",
    "        c.drawString(x+26, y+new_height+35, f\"Expected: {true}\")\n",
    "\n",
    "        # Rótulo predito\n",
    "        c.setFont(\"Helvetica\", 12)\n",
    "        c.setFillColorRGB(*pred_color)  # Verde ou Vermelho\n",
    "        c.drawString(x+26, y+new_height+20, f\"Predicted: {pred}\")\n",
    "\n",
    "        # Rótulo predito\n",
    "        c.setFont(\"Helvetica\", 12)\n",
    "        c.setFillColorRGB(*pred_color)  # Verde ou Vermelho\n",
    "        c.drawString(x+26, y+new_height+5, f\"Prob: {max(y_pred[i])*100:.2f}%\")\n",
    "\n",
    "        # Avançar para a próxima imagem\n",
    "        i += 1\n",
    "        \n",
    "        # Adicionar uma nova página no PDF a cada 2 imagens (se necessário)\n",
    "        if i % 12 == 0:  # Por exemplo, a cada 2 imagens, adicionamos uma nova página\n",
    "            c.showPage()\n",
    "\n",
    "    # Salvar o PDF\n",
    "    c.save()\n",
    "\n",
    "# Exemplo de uso:\n",
    "val_pdf_path = os.path.join(results_dir, \"validation_predictions.pdf\")\n",
    "create_pdf(y_paths, y_true_labels, y_pred_labels, val_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar relatório\n",
    "report = classification_report(y_true_labels, y_pred_labels)\n",
    "print(report)\n",
    "\n",
    "# Escrevendo o relatório em um arquivo .txt\n",
    "with open(os.path.join(results_dir, \"classification_report.txt\"), \"w\") as file:\n",
    "    file.write(report)\n",
    "\n",
    "# Calcular a matriz de confusão\n",
    "cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "\n",
    "# Plotando a matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Previsões')\n",
    "plt.ylabel('Valores Reais')\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.savefig(f'{results_dir}/confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Plotando o histórico de treinamento após o treinamento\n",
    "plot_training_history(history, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(r\"C:\\Users\\Team Taiane\\Desktop\\ADNI\\FULL_ADNI\\processed_7_slices_data\\7_slices_axial\\results\\3d\\axial_certo\\test_5\\binary_classifier_120_epochs_batch_64_5_classes.keras\")\n",
    "\n",
    "test_pdf_generator = nifti_data_generator_pdf(test_image_paths, test_labels, batch_size, shuffle_each_epoch=False)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_image = []\n",
    "y_paths = []\n",
    "\n",
    "for i in range(0, ceil(len(test_image_paths) / batch_size)):\n",
    "    batch_images, batch_labels, batch_paths = next(test_pdf_generator)\n",
    "    y_image.append(batch_images)\n",
    "    y_true.append(batch_labels)\n",
    "    y_paths.append(batch_paths)\n",
    "    \n",
    "    # Fazendo predição para o lote atual\n",
    "    batch_pred = best_model.predict(batch_images)\n",
    "    y_pred.append(batch_pred)\n",
    "\n",
    "# Concatenando as predições e os rótulos verdadeiros\n",
    "y_paths = np.concatenate(y_paths)\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "y_image = np.concatenate(y_image)\n",
    "\n",
    "# Convertendo as predições para rótulos (a classe com maior probabilidade)\n",
    "y_true_labels = np.argmax(y_true, axis=1)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tamanho de y_true_labels: {len(y_true_labels)}\")\n",
    "print(f\"Tamanho de y_pred_labels: {len(y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar relatório\n",
    "report = classification_report(y_true_labels, y_pred_labels)\n",
    "print(report)\n",
    "\n",
    "# Escrevendo o relatório em um arquivo .txt\n",
    "with open(os.path.join(results_dir, \"classification_report.txt\"), \"w\") as file:\n",
    "    file.write(report)\n",
    "\n",
    "# Calcular a matriz de confusão\n",
    "cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "\n",
    "# Plotando a matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Previsões')\n",
    "plt.ylabel('Valores Reais')\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.savefig(f'{results_dir}/confusion_matrix.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
